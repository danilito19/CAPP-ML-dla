MODEL,PARAMETERS,ACCURACY,PRECISION,RECALL,avg-fold-AUC,AUC-stdv
LR,"LogisticRegression(C=1e-05, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,
          penalty='l1', random_state=0, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)",0.95180722891566261,"[ 0.05031447  0.0443038   0.04487179  0.04516129  0.04545455  0.04575163
  0.04605263  0.04666667  0.04697987  0.0472973   0.04761905  0.04794521
  0.04137931  0.04166667  0.04195804  0.04225352  0.04255319  0.04285714
  0.04316547  0.04347826  0.03649635  0.03676471  0.03703704  0.03731343
  0.03759398  0.03816794  0.03846154  0.03875969  0.0390625   0.03937008
  0.03968254  0.04        0.04032258  0.04065041  0.04098361  0.04132231
  0.04166667  0.04201681  0.04237288  0.04273504  0.04310345  0.04347826
  0.04385965  0.04424779  0.04464286  0.04504505  0.04545455  0.04587156
  0.0462963   0.04672897  0.04716981  0.04761905  0.03846154  0.03883495
  0.04938272  0.05        0.05063291  0.05128205  0.05194805  0.05263158
  0.05333333  0.04054054  0.02739726  0.02777778  0.02941176  0.02985075
  0.03030303  0.03076923  0.03125     0.03278689  0.03333333  0.03389831
  0.03448276  0.03508772  0.03571429  0.04        0.04081633  0.04255319
  0.04347826  0.04651163  0.04761905  0.04878049  0.05        0.05128205
  0.05263158  0.05405405  0.05555556  0.05882353  0.06060606  0.0625
  0.06451613  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.        ]","[ 1.     0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875
  0.875  0.875  0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625
  0.625  0.625  0.5    0.5    0.5    0.5    0.5    0.5    0.5    0.5    0.5
  0.375  0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25
  0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25
  0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.     0.     0.
  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
  0.     0.     0.     0.     0.     0.     0.   ]",0.039944474312508575,0.039944474312508575
LR,"LogisticRegression(C=1e-05, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,
          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)",0.95180722891566261,"[ 0.05031447  0.0443038   0.04458599  0.04487179  0.04516129  0.04545455
  0.04575163  0.04605263  0.04635762  0.04666667  0.04697987  0.0472973
  0.04761905  0.04794521  0.04827586  0.04861111  0.04195804  0.04225352
  0.04255319  0.04285714  0.04316547  0.03623188  0.03649635  0.03676471
  0.03703704  0.03731343  0.03759398  0.03787879  0.03816794  0.03846154
  0.03875969  0.0390625   0.03937008  0.03968254  0.04        0.04032258
  0.04065041  0.04098361  0.04132231  0.04166667  0.04201681  0.04237288
  0.04273504  0.04310345  0.04347826  0.04385965  0.04424779  0.04464286
  0.04504505  0.04545455  0.04587156  0.0462963   0.04672897  0.04716981
  0.04761905  0.04807692  0.04854369  0.04901961  0.04950495  0.05
  0.05050505  0.05102041  0.05154639  0.05208333  0.05263158  0.05319149
  0.05376344  0.05434783  0.05494505  0.05555556  0.05617978  0.05681818
  0.04597701  0.04651163  0.04705882  0.04761905  0.04819277  0.04878049
  0.04938272  0.05        0.05063291  0.05128205  0.05194805  0.05263158
  0.05333333  0.05405405  0.04109589  0.04166667  0.02816901  0.02857143
  0.02898551  0.02941176  0.02985075  0.03030303  0.03076923  0.03125
  0.03174603  0.03225806  0.03278689  0.03333333  0.03389831  0.03448276
  0.03508772  0.03571429  0.03636364  0.03703704  0.03773585  0.03846154
  0.03921569  0.04        0.04081633  0.04166667  0.04255319  0.04347826
  0.04444444  0.04545455  0.04651163  0.04761905  0.04878049  0.05
  0.05128205  0.05263158  0.05405405  0.05555556  0.05714286  0.05882353
  0.06060606  0.0625      0.06451613  0.06666667  0.06896552  0.03571429
  0.03703704  0.03846154  0.04        0.04166667  0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.        ]","[ 1.     0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875
  0.875  0.875  0.875  0.875  0.875  0.875  0.75   0.75   0.75   0.75   0.75
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625
  0.625  0.5    0.5    0.5    0.5    0.5    0.5    0.5    0.5    0.5    0.5
  0.5    0.5    0.5    0.5    0.375  0.375  0.25   0.25   0.25   0.25   0.25
  0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25
  0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25
  0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25
  0.25   0.25   0.25   0.25   0.25   0.125  0.125  0.125  0.125  0.125  0.
  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.   ]",0.042324931733640193,0.042324931733640193
LR,"LogisticRegression(C=0.0001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=100,
          multi_class='ovr', n_jobs=-1, penalty='l1', random_state=0,
          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)",0.95180722891566261,"[ 0.05031447  0.0443038   0.04458599  0.04487179  0.04516129  0.04545455
  0.04575163  0.04605263  0.04666667  0.04697987  0.0472973   0.04761905
  0.04794521  0.04137931  0.04166667  0.04195804  0.04225352  0.04255319
  0.04285714  0.04316547  0.04347826  0.03649635  0.03676471  0.03703704
  0.03731343  0.03759398  0.03787879  0.03816794  0.03846154  0.03875969
  0.0390625   0.03937008  0.03968254  0.04        0.04032258  0.04065041
  0.04098361  0.04132231  0.04166667  0.04201681  0.04237288  0.04273504
  0.04310345  0.04347826  0.04385965  0.04424779  0.04464286  0.04504505
  0.04545455  0.04587156  0.0462963   0.04672897  0.04716981  0.04761905
  0.04807692  0.04854369  0.04901961  0.04950495  0.04        0.04040404
  0.04081633  0.04123711  0.04166667  0.04210526  0.04255319  0.04301075
  0.04347826  0.04395604  0.04444444  0.04494382  0.04545455  0.04597701
  0.04651163  0.04705882  0.04761905  0.04819277  0.04878049  0.04938272
  0.05        0.05063291  0.05128205  0.05194805  0.05263158  0.05333333
  0.04054054  0.02739726  0.02777778  0.02816901  0.02857143  0.02898551
  0.02941176  0.02985075  0.03030303  0.03076923  0.03125     0.03174603
  0.03225806  0.03278689  0.03333333  0.03389831  0.03448276  0.03508772
  0.03571429  0.03636364  0.03703704  0.03773585  0.03846154  0.03921569
  0.04        0.04081633  0.04166667  0.04255319  0.04347826  0.04444444
  0.04545455  0.04651163  0.04761905  0.04878049  0.05        0.05128205
  0.05263158  0.05405405  0.05555556  0.05714286  0.05882353  0.06060606
  0.0625      0.06451613  0.03333333  0.03448276  0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.        ]","[ 1.     0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875
  0.875  0.875  0.875  0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.5    0.5    0.5    0.5
  0.5    0.5    0.5    0.5    0.5    0.5    0.5    0.5    0.5    0.5    0.5
  0.5    0.5    0.5    0.5    0.5    0.5    0.5    0.5    0.5    0.5    0.5
  0.375  0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25
  0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25
  0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25
  0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25
  0.125  0.125  0.     0.     0.     0.     0.     0.     0.     0.     0.
  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
  0.     0.     0.     0.     0.     0.     0.     0.   ]",0.040362001204719535,0.040362001204719535
LR,"LogisticRegression(C=0.0001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=100,
          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=0,
          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)",0.95180722891566261,"[ 0.05298013  0.04666667  0.04697987  0.0472973   0.04761905  0.04794521
  0.04827586  0.04861111  0.04895105  0.04929577  0.04964539  0.04285714
  0.04316547  0.04347826  0.04379562  0.04411765  0.04444444  0.04477612
  0.04511278  0.04545455  0.04580153  0.04615385  0.04651163  0.046875
  0.04724409  0.04761905  0.048       0.0483871   0.04878049  0.04918033
  0.04958678  0.04166667  0.04201681  0.04237288  0.04273504  0.04310345
  0.04347826  0.04385965  0.04424779  0.04464286  0.04504505  0.04545455
  0.04587156  0.0462963   0.04672897  0.04716981  0.04761905  0.04807692
  0.04854369  0.04901961  0.04950495  0.05        0.05050505  0.05102041
  0.05154639  0.05208333  0.05263158  0.05319149  0.05376344  0.05434783
  0.05494505  0.05555556  0.05617978  0.05681818  0.05747126  0.05813953
  0.05882353  0.05952381  0.06024096  0.06097561  0.0617284   0.0625
  0.06329114  0.06410256  0.06493506  0.06578947  0.06666667  0.05405405
  0.05479452  0.05555556  0.05633803  0.05714286  0.05797101  0.05882353
  0.05970149  0.04545455  0.04615385  0.046875    0.04761905  0.0483871
  0.04918033  0.05        0.05084746  0.03448276  0.03508772  0.03571429
  0.03636364  0.03703704  0.03773585  0.03846154  0.03921569  0.04
  0.04081633  0.04166667  0.04255319  0.04347826  0.04444444  0.04545455
  0.04651163  0.04761905  0.04878049  0.05        0.05128205  0.05263158
  0.05405405  0.05555556  0.05714286  0.05882353  0.06060606  0.0625
  0.06451613  0.06666667  0.06896552  0.07142857  0.03703704  0.03846154
  0.04        0.04166667  0.04347826  0.04545455  0.04761905  0.05
  0.05263158  0.05555556  0.05882353  0.0625      0.06666667  0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.        ]","[ 1.     0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875
  0.875  0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75
  0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625
  0.625  0.625  0.625  0.625  0.625  0.625  0.5    0.5    0.5    0.5    0.5
  0.5    0.5    0.5    0.375  0.375  0.375  0.375  0.375  0.375  0.375
  0.375  0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25
  0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25
  0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25
  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125
  0.125  0.125  0.125  0.     0.     0.     0.     0.     0.     0.     0.
  0.     0.     0.     0.     0.     0.   ]",0.047797266410061934,0.047797266410061934
LR,"LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,
          penalty='l1', random_state=0, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)",0.95180722891566261,"[ 0.05031447  0.0443038   0.04458599  0.04487179  0.04516129  0.04545455
  0.04575163  0.04605263  0.04635762  0.04666667  0.04697987  0.0472973
  0.04761905  0.04794521  0.04137931  0.04166667  0.04195804  0.04225352
  0.04255319  0.04285714  0.04316547  0.04347826  0.03649635  0.03676471
  0.03703704  0.03731343  0.03759398  0.03787879  0.03816794  0.03846154
  0.03875969  0.0390625   0.03937008  0.03968254  0.04        0.04032258
  0.04065041  0.04098361  0.04132231  0.04166667  0.04201681  0.04237288
  0.04273504  0.04310345  0.04347826  0.04385965  0.04424779  0.04464286
  0.04504505  0.04545455  0.04587156  0.0462963   0.04672897  0.04716981
  0.04761905  0.04807692  0.04854369  0.04901961  0.04950495  0.05
  0.05050505  0.05102041  0.05154639  0.05208333  0.05263158  0.05319149
  0.05376344  0.05434783  0.05494505  0.04444444  0.04494382  0.04545455
  0.04597701  0.04651163  0.04705882  0.04761905  0.04819277  0.04878049
  0.04938272  0.05        0.05063291  0.05128205  0.05194805  0.05263158
  0.05333333  0.05405405  0.04109589  0.02777778  0.02816901  0.02857143
  0.02898551  0.02941176  0.02985075  0.03030303  0.03076923  0.03125
  0.03174603  0.03225806  0.03278689  0.03333333  0.03389831  0.03448276
  0.03508772  0.03571429  0.03636364  0.03703704  0.03773585  0.03846154
  0.03921569  0.04        0.04081633  0.04166667  0.04255319  0.04347826
  0.04444444  0.04545455  0.04651163  0.04761905  0.04878049  0.05
  0.05128205  0.05263158  0.05405405  0.05555556  0.05714286  0.05882353
  0.06060606  0.0625      0.06451613  0.06666667  0.03448276  0.03571429
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.        ]","[ 1.     0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875
  0.875  0.875  0.875  0.875  0.75   0.75   0.75   0.75   0.75   0.75   0.75
  0.75   0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.5    0.5    0.5
  0.5    0.5    0.5    0.5    0.5    0.5    0.5    0.5    0.5    0.5    0.5
  0.5    0.5    0.5    0.375  0.25   0.25   0.25   0.25   0.25   0.25   0.25
  0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25
  0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25
  0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25
  0.25   0.25   0.25   0.125  0.125  0.     0.     0.     0.     0.     0.
  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.   ]",0.041437974020079232,0.041437974020079232
LR,"LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,
          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)",0.95180722891566261,"[ 0.07619048  0.06730769  0.06796117  0.06862745  0.06930693  0.07
  0.07070707  0.07142857  0.07216495  0.07291667  0.07368421  0.07446809
  0.07526882  0.07608696  0.07692308  0.07777778  0.07865169  0.07954545
  0.08045977  0.06976744  0.07058824  0.07142857  0.07228916  0.07317073
  0.07407407  0.075       0.07594937  0.07692308  0.07792208  0.07894737
  0.06666667  0.06756757  0.06849315  0.06944444  0.05633803  0.05714286
  0.05797101  0.05882353  0.05970149  0.06060606  0.06153846  0.0625
  0.06349206  0.0483871   0.04918033  0.05        0.05084746  0.05172414
  0.05263158  0.05357143  0.05454545  0.05555556  0.05660377  0.05769231
  0.05882353  0.04        0.04081633  0.04166667  0.04255319  0.04347826
  0.04444444  0.04545455  0.04651163  0.04761905  0.04878049  0.05
  0.05128205  0.05263158  0.05405405  0.05555556  0.05714286  0.05882353
  0.06060606  0.0625      0.06451613  0.03333333  0.03448276  0.03571429
  0.03703704  0.03846154  0.04        0.04166667  0.04347826  0.04545455
  0.04761905  0.05        0.05263158  0.05555556  0.05882353  0.0625
  0.06666667  0.07142857  0.07692308  0.08333333  0.09090909  0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.        ]","[ 1.     0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875
  0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.75   0.75
  0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.625
  0.625  0.625  0.625  0.5    0.5    0.5    0.5    0.5    0.5    0.5    0.5
  0.5    0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.375
  0.375  0.375  0.375  0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25
  0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25
  0.25   0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125
  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125
  0.125  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.   ]",0.060372019068778315,0.060372019068778315
LR,"LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,
          penalty='l1', random_state=0, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)",0.95180722891566261,"[ 0.06015038  0.0530303   0.05343511  0.05384615  0.05426357  0.0546875
  0.05511811  0.04761905  0.048       0.0483871   0.04878049  0.04918033
  0.04958678  0.05        0.05042017  0.05084746  0.05128205  0.05172414
  0.05217391  0.05263158  0.05309735  0.05357143  0.05405405  0.05454545
  0.05504587  0.05555556  0.05607477  0.05660377  0.05714286  0.05769231
  0.05825243  0.05882353  0.05940594  0.06        0.06060606  0.06122449
  0.06185567  0.0625      0.05263158  0.05319149  0.05376344  0.05434783
  0.05494505  0.05555556  0.05617978  0.05681818  0.05747126  0.05813953
  0.05882353  0.05952381  0.06024096  0.06097561  0.0617284   0.0625
  0.06329114  0.06410256  0.06493506  0.06578947  0.06666667  0.06756757
  0.05479452  0.05555556  0.05633803  0.04285714  0.04347826  0.04411765
  0.04477612  0.04545455  0.04615385  0.046875    0.04761905  0.0483871
  0.04918033  0.05        0.05084746  0.05172414  0.05263158  0.05357143
  0.05454545  0.03703704  0.03773585  0.03846154  0.03921569  0.04
  0.04081633  0.04166667  0.04255319  0.04347826  0.04444444  0.04545455
  0.04651163  0.04761905  0.04878049  0.05        0.05128205  0.05263158
  0.05405405  0.05555556  0.05714286  0.05882353  0.06060606  0.0625
  0.06451613  0.06666667  0.03448276  0.03571429  0.03703704  0.03846154
  0.04        0.04166667  0.04347826  0.04545455  0.04761905  0.05
  0.05263158  0.05555556  0.05882353  0.0625      0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.        ]","[ 1.     0.875  0.875  0.875  0.875  0.875  0.875  0.75   0.75   0.75   0.75
  0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75
  0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75
  0.75   0.75   0.75   0.75   0.75   0.625  0.625  0.625  0.625  0.625
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.5    0.5    0.5    0.375
  0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.375
  0.375  0.375  0.375  0.375  0.375  0.25   0.25   0.25   0.25   0.25   0.25
  0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25
  0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.125  0.125
  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125
  0.125  0.125  0.     0.     0.     0.     0.     0.     0.     0.     0.
  0.     0.     0.     0.     0.     0.   ]",0.050461562494070265,0.050461562494070265
LR,"LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,
          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)",0.95180722891566261,"[ 0.08695652  0.07692308  0.07777778  0.07865169  0.07954545  0.08045977
  0.08139535  0.08235294  0.08333333  0.08433735  0.08536585  0.08641975
  0.0875      0.08860759  0.08974359  0.07792208  0.07894737  0.08
  0.08108108  0.08219178  0.08333333  0.08450704  0.08571429  0.08695652
  0.08823529  0.08955224  0.09090909  0.09230769  0.09375     0.0952381
  0.09677419  0.09836066  0.1         0.10169492  0.10344828  0.10526316
  0.10714286  0.10909091  0.11111111  0.11320755  0.09615385  0.09803922
  0.1         0.10204082  0.08333333  0.08510638  0.08695652  0.08888889
  0.09090909  0.09302326  0.0952381   0.07317073  0.075       0.07692308
  0.05263158  0.05405405  0.05555556  0.05714286  0.02941176  0.03030303
  0.03125     0.03225806  0.03333333  0.03448276  0.03571429  0.03703704
  0.03846154  0.04        0.04166667  0.04347826  0.04545455  0.04761905
  0.05        0.05263158  0.05555556  0.05882353  0.0625      0.06666667
  0.07142857  0.07692308  0.08333333  0.09090909  0.1         0.11111111
  0.125       0.14285714  0.16666667  0.2         0.25        0.33333333
  0.          0.        ]","[ 1.     0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875
  0.875  0.875  0.875  0.875  0.875  0.75   0.75   0.75   0.75   0.75   0.75
  0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75
  0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.625  0.625
  0.625  0.625  0.5    0.5    0.5    0.5    0.5    0.5    0.5    0.375
  0.375  0.375  0.25   0.25   0.25   0.25   0.125  0.125  0.125  0.125
  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125
  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125
  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.     0.   ]",0.092526893130809015,0.092526893130809015
LR,"LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,
          penalty='l1', random_state=0, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)",0.95180722891566261,"[ 0.07843137  0.06930693  0.07        0.07070707  0.07142857  0.07216495
  0.07291667  0.07368421  0.07446809  0.07526882  0.07608696  0.07692308
  0.07777778  0.07865169  0.07954545  0.08045977  0.08139535  0.08235294
  0.08333333  0.08433735  0.08536585  0.08641975  0.0875      0.08860759
  0.08974359  0.07792208  0.07894737  0.08        0.06756757  0.06849315
  0.06944444  0.07042254  0.07142857  0.07246377  0.07352941  0.07462687
  0.07575758  0.07692308  0.078125    0.07936508  0.06451613  0.06557377
  0.06666667  0.05084746  0.05172414  0.05263158  0.05357143  0.05454545
  0.05555556  0.05660377  0.05769231  0.05882353  0.04        0.04081633
  0.04166667  0.04255319  0.04347826  0.04444444  0.04545455  0.04651163
  0.04761905  0.04878049  0.05        0.05128205  0.05263158  0.05405405
  0.05555556  0.02857143  0.02941176  0.03030303  0.03125     0.03225806
  0.03333333  0.03448276  0.03571429  0.03703704  0.03846154  0.04
  0.04166667  0.04347826  0.04545455  0.04761905  0.05        0.05263158
  0.05555556  0.05882353  0.0625      0.06666667  0.07142857  0.07692308
  0.08333333  0.09090909  0.1         0.11111111  0.125       0.          0.
  0.          0.          0.          0.          0.        ]","[ 1.     0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875
  0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875
  0.875  0.875  0.875  0.875  0.875  0.75   0.75   0.75   0.625  0.625
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.5
  0.5    0.5    0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.375
  0.375  0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25   0.25
  0.25   0.25   0.25   0.25   0.25   0.125  0.125  0.125  0.125  0.125
  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125
  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125
  0.125  0.125  0.125  0.     0.     0.     0.     0.     0.     0.   ]",0.064186723905294077,0.064186723905294077
LR,"LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,
          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)",0.95180722891566261,"[ 0.17391304  0.15555556  0.15909091  0.1627907   0.16666667  0.17073171
  0.175       0.17948718  0.18421053  0.16216216  0.16666667  0.17142857
  0.17647059  0.18181818  0.15625     0.16129032  0.16666667  0.17241379
  0.17857143  0.14814815  0.15384615  0.12        0.125       0.13043478
  0.13636364  0.14285714  0.15        0.15789474  0.16666667  0.17647059
  0.1875      0.2         0.21428571  0.23076923  0.25        0.27272727
  0.3         0.33333333  0.375       0.42857143  0.5         0.6         0.5
  0.66666667  1.          1.        ]","[ 1.     0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.75   0.75
  0.75   0.75   0.75   0.625  0.625  0.625  0.625  0.625  0.5    0.5    0.375
  0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.375
  0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.375
  0.25   0.25   0.25   0.125]",0.25567296260752576,0.25567296260752576
LR,"LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,
          penalty='l1', random_state=0, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)",0.95180722891566261,"[ 0.13559322  0.12068966  0.12280702  0.125       0.12727273  0.12962963
  0.13207547  0.13461538  0.1372549   0.14        0.14285714  0.14583333
  0.14893617  0.15217391  0.15555556  0.15909091  0.1627907   0.16666667
  0.17073171  0.175       0.15384615  0.13157895  0.13513514  0.13888889
  0.14285714  0.14705882  0.15151515  0.15625     0.16129032  0.16666667
  0.17241379  0.14285714  0.14814815  0.15384615  0.16        0.16666667
  0.17391304  0.18181818  0.14285714  0.15        0.15789474  0.16666667
  0.17647059  0.1875      0.2         0.21428571  0.23076923  0.25
  0.27272727  0.3         0.33333333  0.375       0.42857143  0.33333333
  0.4         0.5         0.33333333  0.5         1.        ]","[ 1.     0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875
  0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875
  0.75   0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625
  0.625  0.5    0.5    0.5    0.5    0.5    0.5    0.5    0.375  0.375
  0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.375
  0.375  0.375  0.375  0.25   0.25   0.25   0.125  0.125  0.125]",0.19323628304637785,0.19323628304637785
LR,"LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,
          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)",0.95180722891566261,"[ 0.17777778  0.15909091  0.13953488  0.14285714  0.14634146  0.15
  0.15384615  0.15789474  0.16216216  0.16666667  0.17142857  0.17647059
  0.18181818  0.1875      0.16129032  0.16666667  0.17241379  0.17857143
  0.14814815  0.11538462  0.12        0.125       0.13043478  0.13636364
  0.14285714  0.15        0.15789474  0.16666667  0.17647059  0.1875      0.2
  0.21428571  0.23076923  0.25        0.27272727  0.3         0.33333333
  0.375       0.42857143  0.5         0.6         0.5         0.66666667
  0.5         0.        ]","[ 1.     0.875  0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75
  0.75   0.75   0.75   0.625  0.625  0.625  0.625  0.5    0.375  0.375
  0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.375
  0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.375
  0.25   0.25   0.125  0.   ]",0.30134274914810333,0.30134274914810333
LR,"LogisticRegression(C=5, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,
          penalty='l1', random_state=0, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)",0.95783132530120485,"[ 0.17021277  0.15217391  0.15555556  0.15909091  0.1627907   0.16666667
  0.17073171  0.175       0.17948718  0.15789474  0.16216216  0.16666667
  0.17142857  0.17647059  0.18181818  0.1875      0.19354839  0.2
  0.20689655  0.21428571  0.22222222  0.23076923  0.24        0.25
  0.26086957  0.27272727  0.28571429  0.3         0.31578947  0.27777778
  0.23529412  0.25        0.26666667  0.28571429  0.23076923  0.25
  0.27272727  0.2         0.22222222  0.25        0.28571429  0.33333333
  0.4         0.5         0.66666667  1.          1.        ]","[ 1.     0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.75   0.75
  0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75
  0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.625  0.5    0.5    0.5
  0.5    0.375  0.375  0.375  0.25   0.25   0.25   0.25   0.25   0.25   0.25
  0.25   0.25   0.125]",0.29674325003363311,0.29674325003363311
LR,"LogisticRegression(C=5, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,
          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)",0.95783132530120485,"[ 0.13333333  0.11864407  0.12068966  0.12280702  0.125       0.12727273
  0.12962963  0.13207547  0.13461538  0.1372549   0.14        0.14285714
  0.14583333  0.14893617  0.15217391  0.15555556  0.15909091  0.1627907
  0.16666667  0.17073171  0.175       0.17948718  0.18421053  0.18918919
  0.16666667  0.17142857  0.17647059  0.18181818  0.1875      0.16129032
  0.13333333  0.13793103  0.14285714  0.11111111  0.11538462  0.12        0.125
  0.13043478  0.13636364  0.14285714  0.15        0.15789474  0.16666667
  0.17647059  0.1875      0.2         0.21428571  0.23076923  0.25
  0.27272727  0.2         0.22222222  0.25        0.28571429  0.33333333
  0.4         0.5         0.66666667  0.5         0.        ]","[ 1.     0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875
  0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875  0.875
  0.875  0.875  0.875  0.875  0.75   0.75   0.75   0.75   0.75   0.625  0.5
  0.5    0.5    0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.375
  0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.375  0.25   0.25
  0.25   0.25   0.25   0.25   0.25   0.25   0.125  0.   ]",0.27780849121757362,0.27780849121757362
NB,GaussianNB(),0.42771084337349397,"[ 0.04819277  0.05217391  0.05263158  0.05309735  0.05357143  0.05405405
  0.05454545  0.05504587  0.05555556  0.05607477  0.05660377  0.05714286
  0.05769231  0.05825243  0.05882353  0.05940594  0.06        0.06060606
  0.06122449  0.05154639  0.05208333  0.05263158  0.05319149  0.05376344
  0.05434783  0.05494505  0.05555556  0.05617978  0.05681818  0.05747126
  0.05813953  0.05882353  0.05952381  0.06024096  0.06097561  0.0617284
  0.0625      0.06329114  0.06410256]","[ 1.     0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75
  0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.75   0.625  0.625
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625
  0.625  0.625  0.625  0.625  0.625  0.625  0.625  0.625]",0.019495957718426885,0.019495957718426885
